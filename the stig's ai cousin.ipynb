{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Hybrid Approach using PlanSearch and Contrastive Decoding\n",
    "\n",
    "X: https://x.com/_aloobun\n",
    "\n",
    "> Project 2: Generate the funniest joke imaginable with LLMs.\n",
    "> Implement PlanSearch but for generating jokes, then use LLM as a judge to rate generated jokes and show top ones. Your pipeline should input a word or a context (like “penguins” or “nodejs locked in a VM”) and output top funniest jokes.Please note that doing LLM-as-a-judge correctly is a tricky thing as they have all sorts of biases in rating or position preferences. So the expectation is that you will read literature on it beforehand.Optional follow up: think and answer how would you test whether generated jokes are truly novel? What’s novelty in any case? What if LLMs are just memorizing? How would you test it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prelude**: Whenever I think of language models, I think of something like open fields rather than closed roads. I named the file as The Stig’s AI cousin because he had one of the funniest introductions on the show, and one of them was, “His favourite flower is the potato”.\n",
    "\n",
    "**Why did you pick the particular project?**\n",
    "\n",
    "One of the main reasons I wanted to work on this is, if we’re not solving the creativity/diversity part, we are not allowing language models to progress. It’s interesting to see, when prompted directly, llm tend to produce derivative content (good memorization i guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `allow` has been saved to /teamspace/studios/this_studio/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /teamspace/studios/this_studio/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `allow`\n"
     ]
    }
   ],
   "source": [
    "##o\n",
    "#!pip install transformer_lens\n",
    "#!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the 'problem' before proposing a solution\n",
    "\n",
    "I **hypothesize** that for any given topic, the model has a default internal pathway for simple, factual statements and a distinct, separate pathway for creative outputs (lol, i was wrong and how!!)**. We can identify and map these pathways using causal tracing. \n",
    "\n",
    "**And sometimes there will be overlapping circuits (i can deep dive into what overlapping circuits mean, do they memorize ??) - you'd see this in the problem finding interp experiment.\n",
    "\n",
    "It seems like we have to somehow steer llm to be creative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding circuits\n",
      "\n",
      "Finding the 'creative' circuit\n",
      "  Baseline log prob for target ' 4, but it’s a lot more complicated when you’re trying to split the bill.': -447.70\n",
      "\n",
      "Finding the 'descriptive' circuit\n",
      "  Baseline log prob for target ' 4.': -39.37\n",
      "\n",
      "'Creative' Circuit\n",
      "Top 10 important components overall:\n",
      "    component  improvement\n",
      "0       MLP_0   225.454651\n",
      "495    MLP_15   196.012558\n",
      "396    MLP_12   182.134460\n",
      "330    MLP_10   174.880646\n",
      "264     MLP_8   164.607544\n",
      "363    MLP_11   158.541931\n",
      "297     MLP_9   158.233582\n",
      "231     MLP_7   155.855927\n",
      "429    MLP_13   124.825500\n",
      "462    MLP_14   124.810913\n",
      "\n",
      "MLP components: ['MLP_0', 'MLP_15', 'MLP_12', 'MLP_10', 'MLP_8']\n",
      "\n",
      "'Descriptive' Circuit\n",
      "Top 10 important components overall:\n",
      "    component  improvement\n",
      "0       MLP_0    12.455952\n",
      "330    MLP_10     8.141054\n",
      "264     MLP_8     7.143742\n",
      "363    MLP_11     6.051708\n",
      "429    MLP_13     5.829998\n",
      "396    MLP_12     5.126740\n",
      "297     MLP_9     5.102928\n",
      "198     MLP_6     3.095867\n",
      "132     MLP_4     3.022987\n",
      "231     MLP_7     2.800716\n",
      "\n",
      "MLP components: ['MLP_0', 'MLP_10', 'MLP_8', 'MLP_11', 'MLP_13']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from functools import partial\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class CircuitFinder:\n",
    "    def __init__(self, model: HookedTransformer):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        if self.model.tokenizer.pad_token is None:\n",
    "            self.model.tokenizer.pad_token = self.model.tokenizer.eos_token\n",
    "\n",
    "    def _tokenize_with_chat_template(self, text, system_prompt=None):\n",
    "        if system_prompt:\n",
    "            messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": text}]\n",
    "        else:\n",
    "            messages = [{\"role\": \"user\", \"content\": text}]\n",
    "        # Using the model's tokenizer directly\n",
    "        return self.model.tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=False, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "    def _patch_full_activation_hook(self, activation, hook, clean_activation):\n",
    "        activation[:, :, :] = clean_activation[:, :, :]\n",
    "        return activation\n",
    "\n",
    "    def _patch_head_hook(self, activation, hook, clean_head_activation, head_index):\n",
    "        activation[:, :, head_index, :] = clean_head_activation\n",
    "        return activation\n",
    "\n",
    "    def _calculate_target_log_prob(self, logits, target_tokens, seq_len):\n",
    "        punchline_len = target_tokens.shape[1]\n",
    "        start_index = seq_len - punchline_len\n",
    "        end_index = seq_len\n",
    "        relevant_logits = logits[0, start_index-1:end_index-1, :]\n",
    "        log_probs = torch.nn.functional.log_softmax(relevant_logits, dim=-1)\n",
    "        # squeeze target_tokens to be 1D\n",
    "        target_indices = target_tokens.squeeze(0)\n",
    "        return log_probs[torch.arange(punchline_len), target_indices].sum()\n",
    "\n",
    "    def run_patching_experiment(self, clean_text, corrupted_text, target_punchline_text, system_prompt=None):\n",
    "        clean_tokens = self._tokenize_with_chat_template(clean_text, system_prompt)\n",
    "        corrupted_tokens = self._tokenize_with_chat_template(corrupted_text, system_prompt)\n",
    "        target_tokens = self.model.to_tokens(target_punchline_text, prepend_bos=False).to(device)\n",
    "        \n",
    "        len_clean, len_corr = clean_tokens.shape[1], corrupted_tokens.shape[1]\n",
    "        max_len = max(len_clean, len_corr)\n",
    "        \n",
    "        if len_clean < max_len:\n",
    "            padding = torch.full((1, max_len - len_clean), self.model.tokenizer.pad_token_id, device=device)\n",
    "            clean_tokens = torch.cat([clean_tokens, padding], dim=1)\n",
    "        if len_corr < max_len:\n",
    "            padding = torch.full((1, max_len - len_corr), self.model.tokenizer.pad_token_id, device=device)\n",
    "            corrupted_tokens = torch.cat([corrupted_tokens, padding], dim=1)\n",
    "        \n",
    "        seq_len = max_len\n",
    "\n",
    "        # get baseline and cache\n",
    "        _, clean_cache = self.model.run_with_cache(clean_tokens)\n",
    "        corrupted_logits = self.model(corrupted_tokens)\n",
    "        baseline_log_prob = self._calculate_target_log_prob(corrupted_logits, target_tokens, seq_len)\n",
    "        print(f\"  Baseline log prob for target '{target_punchline_text}': {baseline_log_prob.item():.2f}\")\n",
    "\n",
    "        # patching experiments\n",
    "        results = []\n",
    "        for layer in range(self.model.cfg.n_layers):\n",
    "            # Patch MLP Layers\n",
    "            mlp_hook_name = utils.get_act_name(\"post\", layer, \"mlp\")\n",
    "            clean_mlp_activation = clean_cache[mlp_hook_name]\n",
    "            hook_fn = partial(self._patch_full_activation_hook, clean_activation=clean_mlp_activation)\n",
    "            with self.model.hooks(fwd_hooks=[(mlp_hook_name, hook_fn)]):\n",
    "                patched_logits_mlp = self.model(corrupted_tokens)\n",
    "            improvement_mlp = self._calculate_target_log_prob(patched_logits_mlp, target_tokens, seq_len).item() - baseline_log_prob.item()\n",
    "            results.append({\"component\": f\"MLP_{layer}\", \"improvement\": improvement_mlp})\n",
    "            \n",
    "            # patch attention heads\n",
    "            for head in range(self.model.cfg.n_heads):\n",
    "                attn_hook_name = utils.get_act_name(\"z\", layer)\n",
    "                clean_attn_activation = clean_cache[attn_hook_name][:, :, head, :]\n",
    "                hook_fn_attn = partial(self._patch_head_hook, clean_head_activation=clean_attn_activation, head_index=head)\n",
    "                with self.model.hooks(fwd_hooks=[(attn_hook_name, hook_fn_attn)]):\n",
    "                    patched_logits_attn = self.model(corrupted_tokens)\n",
    "                improvement_attn = self._calculate_target_log_prob(patched_logits_attn, target_tokens, seq_len).item() - baseline_log_prob.item()\n",
    "                results.append({\"component\": f\"L{layer}H{head}\", \"improvement\": improvement_attn})\n",
    "\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Finding circuits\")\n",
    "\n",
    "    circuit_finder = CircuitFinder(model=model)\n",
    "\n",
    "    my_system_prompt = \"You are a helpful assistant and you will give funny replies like a standup comedian.\"\n",
    "    \n",
    "    context_only = \"What is 2+2?\"\n",
    "    descriptive_completion_text = \"What is 2+2? 4.\"\n",
    "    creative_completion_text = \"What is 2+2? 4, but it’s a lot more complicated when you’re trying to split the bill.\"\n",
    "    descriptive_punchline_text = \" 4.\"\n",
    "    creative_punchline_text = \" 4, but it’s a lot more complicated when you’re trying to split the bill.\"\n",
    "\n",
    "\n",
    "    print(\"\\nFinding the 'creative' circuit\")\n",
    "    df_creative_circuit_full = circuit_finder.run_patching_experiment(\n",
    "        clean_text=creative_completion_text, \n",
    "        corrupted_text=descriptive_completion_text, \n",
    "        target_punchline_text=creative_punchline_text,\n",
    "        system_prompt=my_system_prompt\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(\"\\nFinding the 'descriptive' circuit\")\n",
    "    df_descriptive_circuit_full = circuit_finder.run_patching_experiment(\n",
    "        clean_text=descriptive_completion_text, \n",
    "        corrupted_text=creative_completion_text, \n",
    "        target_punchline_text=descriptive_punchline_text,\n",
    "        system_prompt=my_system_prompt\n",
    "    )\n",
    "\n",
    "    print(\"\\n'Creative' Circuit\")\n",
    "    print(\"Top 10 important components overall:\")\n",
    "    print(df_creative_circuit_full.sort_values(by=\"improvement\", ascending=False).head(10).to_string())\n",
    "    creative_circuit_components = df_creative_circuit_full[df_creative_circuit_full.component.str.startswith('MLP')].sort_values(\n",
    "        by=\"improvement\", ascending=False\n",
    "    ).head(5)['component'].tolist()\n",
    "    print(f\"\\nMLP components: {creative_circuit_components}\")\n",
    "\n",
    "    print(\"\\n'Descriptive' Circuit\")\n",
    "    print(\"Top 10 important components overall:\")\n",
    "    print(df_descriptive_circuit_full.sort_values(by=\"improvement\", ascending=False).head(10).to_string())\n",
    "    descriptive_circuit_components = df_descriptive_circuit_full[df_descriptive_circuit_full.component.str.startswith('MLP')].sort_values(\n",
    "        by=\"improvement\", ascending=False\n",
    "    ).head(5)['component'].tolist()\n",
    "    print(f\"\\nMLP components: {descriptive_circuit_components}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Solution' \n",
    "## PlanSearch was boring so i extended it's idea with contrastive sampling.\n",
    "\n",
    "## Explanation\n",
    "\n",
    "We feed a detailed prompt (as explained in Appendix [here](https://arxiv.org/pdf/2409.03733)) into the model, model processes the prompt and calculates the probability for every single word, instead of using all the observations at once, `PlanSearch` creates many small, separate branches to explore different combinations of ideas, iterates each branch and the `PlanSearch` algo takes those logits (flattens it slightly) which makes less likely words a bit more likely to be chosen.\n",
    "\n",
    "### My Extension to PlanSearch\n",
    "Here the core of my extension to PlanSearch comes from paper [Dola](https://arxiv.org/abs/2309.03883), where the core idea is to take two models with same vocab and feed same context/question and instead of predicting the next token one at a time, we look difference of logits from from bigger and smaller models respectively and accotdingly we interpolate (to steer the generation process - as said above that we need to steer). The core of the algo is to calculate this:\n",
    "`final_logits = (1 - alpha) * logits1 + alpha * logits2` \n",
    "\n",
    "Here `alpha` is a scalar, 0 means take logits from smaller model, 1 means take logits from bigger model. `logits1` is logits from smaller model and `logits2` is logits from bigger model.\n",
    "\n",
    "As per my **hypothesis** that: for any given topic, the model has a default internal pathway for simple, factual statements and a distinct, separate pathway for creative outputs. We can identify and map these pathways using causal tracing, this approach felt better and useful than writing detailed prompts. However, initial results are okaish, and i can work on this again to make it better (i guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7bde0e176b4cd093989fa77509992f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Contrastive Generated Jokes (Using Fixed Alpha = 1.2, Count = 5) ---\n",
      "\n",
      "Joke #1\n",
      "  Angle: \"Gym Profiling: Why they know exactly what brand of socks you wear and what type of cardio machine you're too intimidated to use.\"\n",
      "  Joke: They must have a'sole'- search algorithm to figure out why you're still on the Treadmill of Shame.\n",
      "\n",
      "Joke #2\n",
      "  Angle: \"Bulkedup bullies: The gym is full of selfproclaimed 'tough guys' who can bench press a small car, but still manage to ask where the 'long' toilet seat is.\"\n",
      "  Joke: Why did the gym-goer who could bench 500 pounds walk into a bathroom with a long toilet seat? Because even the strongest guys need to answer nature's biggest question.\n",
      "\n",
      "Joke #3\n",
      "  Angle: \"Gym Etiquette 101: Don't even think about pulling out that smartphone in the squat rack – unless you're documenting your own midworkout existential crisis.\"\n",
      "  Joke: Here's a joke based on the angle:\n",
      "\n",
      "\"I was trying to squat in peace, but then I saw someone live-streaming their inner turmoil on Instagram... and I'm like, 'At least you're consistent, dude, you're crushing those feelings just like you crush those weights.'\"\n",
      "\n",
      "Joke #4\n",
      "  Angle: \"Gym Mythbusting: The only thing that's 'gu\n",
      "  Joke: Why did the treadmill go to therapy? Because it was struggling to 'gu'arantee a good workout.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class ContrastiveJokeSearch:\n",
    "    def __init__(self, model1: AutoModelForCausalLM, model2: AutoModelForCausalLM, tokenizer: AutoTokenizer, system_prompt: str, alpha: float):\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_prompt = system_prompt\n",
    "        self.alpha = alpha #fixed alpha value that will be used for all generation steps\n",
    "        \n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "            \n",
    "    def _get_next_token_logits(self, model, input_ids, past_key_values): # probabilities (logits) for the very next token.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, past_key_values=past_key_values, use_cache=True, return_dict=True)\n",
    "        return outputs.logits[:, -1, :], outputs.past_key_values\n",
    "\n",
    "    # generates a sequence of text by combining the \"thoughts\" of two different models.\n",
    "    def _contrastive_generate(self, messages: List[Dict], max_new_tokens: int = 128, temperature: float = 0.7) -> str: \n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            messages, add_generation_prompt=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        \n",
    "        generated_ids = []\n",
    "        _, past_key_values1 = self._get_next_token_logits(self.model1, input_ids, None)\n",
    "        _, past_key_values2 = self._get_next_token_logits(self.model2, input_ids, None)\n",
    "        current_input_ids = input_ids[:, -1:]\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            #in each loop, get the next-token logits for smaller model\n",
    "            logits1, past_key_values1 = self._get_next_token_logits(self.model1, current_input_ids, past_key_values=past_key_values1)\n",
    "            #do the same for larger model\n",
    "            logits2, past_key_values2 = self._get_next_token_logits(self.model2, current_input_ids, past_key_values=past_key_values2)\n",
    "\n",
    "            ## this is the core of the solution here\n",
    "            # it combines the logits from both models using the class's single `alpha` value.\n",
    "            final_logits = (1.0 - self.alpha) * logits1 + self.alpha * logits2\n",
    "\n",
    "            if temperature > 0:\n",
    "                next_token_id = torch.multinomial(torch.softmax(final_logits / temperature, dim=-1), num_samples=1)\n",
    "            else:\n",
    "                next_token_id = torch.argmax(final_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            if next_token_id.item() == self.tokenizer.eos_token_id: break\n",
    "            generated_ids.append(next_token_id.item())\n",
    "            current_input_ids = next_token_id\n",
    "\n",
    "        return self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    def _parse_list_from_response(self, response_text: str) -> List[str]:\n",
    "        items = response_text.strip().split('\\n')\n",
    "        cleaned_items = [re.sub(r'^\\s*\\d+[\\.\\)]\\s*|\\s*-\\s*', '', item).strip() for item in items]\n",
    "        return [item for item in cleaned_items if item]\n",
    "\n",
    "    #first step of the PlanSearch\n",
    "    def generate_observations(self, topic: str, num_observations: int = 5) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the topic: \"{topic}\". Generate a numbered list of {num_observations} humorous observations.\n",
    "        Each observation should be on a new line.\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        logging.info(f\"Generating observations with contrastive decoding (alpha={self.alpha})...\")\n",
    "        response_text = self._contrastive_generate(messages)\n",
    "        return self._parse_list_from_response(response_text)\n",
    "\n",
    "        #second step of the PlanSearch\n",
    "    def generate_comedic_angles(self, topic: str, observations: List[str], num_angles: int = 3) -> List[str]:\n",
    "        prompt = f\"\"\"\n",
    "        Given these observations: {json.dumps(observations)}.\n",
    "        Generate a numbered list of {num_angles} unique comedic angles for the topic \"{topic}\".\n",
    "        Each angle should be on a new line.\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        logging.info(f\"Generating comedic angles with contrastive decoding (alpha={self.alpha})...\")\n",
    "        response_text = self._contrastive_generate(messages)\n",
    "        return self._parse_list_from_response(response_text)\n",
    "\n",
    "    #final step of PlanSeach\n",
    "    def write_final_joke(self, topic: str, angle: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        Write a single, concise joke for the topic \"{topic}\" based on this angle: \"{angle}\".\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        logging.info(f\"Writing final joke with contrastive decoding (alpha={self.alpha}) for angle: '{angle}'\")\n",
    "        return self._contrastive_generate(messages, max_new_tokens=64)\n",
    "\n",
    "    def run_full_pipeline(self, topic: str, max_results: int = 5) -> List[Dict[str, str]]:\n",
    "        logging.info(f\"Starting Contrastive Pipeline for topic: '{topic}' (Strict max results: {max_results}) ---\")\n",
    "        \n",
    "        observations = self.generate_observations(topic, num_observations=5)\n",
    "        if not observations:\n",
    "            return []\n",
    "        \n",
    "        angles = self.generate_comedic_angles(topic, observations, num_angles=max_results + 2)\n",
    "        if not angles:\n",
    "            return []\n",
    "        \n",
    "        angles_to_use = angles[:max_results]\n",
    "        logging.info(f\"Model generated {len(angles)} angles, but we will use the top {len(angles_to_use)} to meet the max_results limit.\")\n",
    "        \n",
    "        results = [{\"angle\": angle, \"joke\": self.write_final_joke(topic, angle)} for angle in angles_to_use]\n",
    "        return [res for res in results if res[\"joke\"]]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MODEL1_ID = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "    MODEL2_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "    logging.info(f\"Loading smaller model: {MODEL1_ID}\")\n",
    "    model1 = AutoModelForCausalLM.from_pretrained(MODEL1_ID, torch_dtype=torch.bfloat16).to(DEVICE).eval()\n",
    "    \n",
    "    logging.info(f\"Loading bigger model: {MODEL2_ID}\")\n",
    "    model2 = AutoModelForCausalLM.from_pretrained(MODEL2_ID, torch_dtype=torch.bfloat16).to(DEVICE).eval()\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL1_ID)\n",
    "\n",
    "    SYSTEM_PROMPT = \"You are a witty and creative comedian.\"\n",
    "    JOKE_TOPIC = \"the gym\"\n",
    "\n",
    "    ALPHA_SETTING = 1.2 # 0 is for smaller model, 1 for larger, interestgint to see what comes of when >1 is used\n",
    "    MAX_JOKES_TO_GENERATE = 5\n",
    "\n",
    "    pipeline = ContrastiveJokeSearch(\n",
    "        model1=model1, \n",
    "        model2=model2, \n",
    "        tokenizer=tokenizer, \n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        alpha=ALPHA_SETTING\n",
    "    )\n",
    "    \n",
    "    generated_jokes = pipeline.run_full_pipeline(topic=JOKE_TOPIC, max_results=MAX_JOKES_TO_GENERATE)\n",
    "\n",
    "    print(f\"\\n\\n--- Contrastive Generated Jokes (Using Fixed Alpha = {ALPHA_SETTING}, Count = {MAX_JOKES_TO_GENERATE}) ---\")\n",
    "    if generated_jokes:\n",
    "        for i, result in enumerate(generated_jokes):\n",
    "            print(f\"\\nJoke #{i+1}\\n  Angle: {result['angle']}\\n  Joke: {result['joke']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did you pick the particular project?\n",
    "\n",
    "One of the main reasons I wanted to work on this is, if we’re not solving the creativity/diversity part, we are not allowing language models to progress. It’s interesting to see, when prompted directly, llm tend to produce derivative content (good memorization i guess).\n",
    "\n",
    "### What I Learned From This Project\n",
    "\n",
    "The approach I've used(not perfect imo) isn't just a fancy way to generate textmusing glorified prompts. It's a direct method for changing the model's decision making at the most fundamental level which is the logits. Something like \"creativity\" as a mathematical direction in the latent space. My takeaway is that both the small model and the large model have likely learned many of the same fundamental circuits for basic grammar, concepts, and common sense, mainly becuase of the polysemantic neurons capture and compress all different type of features leading to more generic outputs.\n",
    "\n",
    "### What surprised you the most?\n",
    "\n",
    "There's a joke in here somewhere! \n",
    "\n",
    "### If you had more compute, what would you have done?\n",
    "Right now, our alpha parameter amplifies everything that the larger model does differently. With more time or compute probably i would train a suite of linear probes. Each probe would be a simple classifier trained on the model's internal activations, having said that, i realise after writing that probe only shows a correlation so may be a natural progression from there would be to prove causation. Still open-ended but it would be an interesting thing to do.\n",
    "\n",
    "### If you had to write a paper on the project, what else needs to be done?\n",
    "the first step would be to prove our jokes are actually funny, which means moving beyond just my own biased opinion (human vs llm as a judge may be?? but it has itws own pros and cons w.r.t internal biases) and to make sure we're not just creating a more elaborate parrot, i'd need to provide mechanistic evidence, using causal tracing to show our jokes rely less on simple pattern-matching circuits and more on... well, that's it for now.\n",
    "\n",
    "This was fun, thank you for making me think around diversity/creativity issue with llm. GLHF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
